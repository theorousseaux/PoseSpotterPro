{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_joints_dict = {0: 'nose',\n",
    "        1: 'left_eye',\n",
    "        2: 'right_eye',\n",
    "        3: 'left_ear',\n",
    "        4: 'right_ear',\n",
    "        5: 'left_shoulder',\n",
    "        6: 'right_shoulder',\n",
    "        7: 'left_elbow',\n",
    "        8: 'right_elbow',\n",
    "        9: 'left_wrist',\n",
    "        10: 'right_wrist',\n",
    "        11: 'left_hip',\n",
    "        12: 'right_hip',\n",
    "        13: 'left_knee',\n",
    "        14: 'right_knee',\n",
    "        15: 'left_ankle',\n",
    "        16: 'right_ankle'}\n",
    "joints_id_dict = {v: k for k, v in id_joints_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_action(action_filename):\n",
    "    '''\n",
    "    Transform the name of an action file into the format \"Subject.X/Action\"\n",
    "\n",
    "    Args:\n",
    "        action_filename (str): the name of the action file\n",
    "        \n",
    "    Returns:\n",
    "        str: the path to the action file in the format \"Subject.X/Action\"\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Récupérer le numéro de sujet\n",
    "    subject_number = re.search(r'S(\\d+).json$', action_filename).group(1) # group(1) pour récupérer le premier groupe de la regex, qui correspond au numéro de sujet car c'est ce qui est entre parenthèses\n",
    "    # Supprimer l'extension .json et le \"S\" suivi du numéro à la fin\n",
    "    action = re.sub(r'S\\d+.json$', '', action_filename)\n",
    "    # Cas d'exception pour \"Pickupobject\"\n",
    "    if action == \"Pickupobject\":\n",
    "        formatted_action = \"Pick up object\"\n",
    "    else:\n",
    "        # Séparer les mots en utilisant les majuscules comme séparateur\n",
    "        words = re.findall('[A-Z][^A-Z]*', action)\n",
    "        # Mettre la première en minuscule, sauf pour le premier mot\n",
    "        words = [words[0]] + [word[0].lower() + word[1:] for word in words[1:]]\n",
    "        # Joindre les mots avec un espace\n",
    "        formatted_action = ' '.join(words)\n",
    "    return f'Subject.{subject_number}/{formatted_action}'\n",
    "\n",
    "# Testons le script avec vos exemples\n",
    "print(format_action('FallBackwardsS1.json'))  # 'Subject.1/Fall backwards'\n",
    "print(format_action('HopS2.json'))  # 'Subject.2/Hop'\n",
    "print(format_action('PickupobjectS3.json'))  # 'Subject.3/Pick up object'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info_dict(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "    codec = \"\".join([chr((fourcc >> 8 * i) & 0xFF) for i in range(4)])\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    duration = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) / int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    cap.release()\n",
    "    return {'num_frames': num_frames, 'fps': fps, 'codec': codec, 'width': width, 'height': height, 'duration': duration}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "list_dir = os.listdir('../../outputs/fall/bbox_imposed/predictions/')\n",
    "list_dir.sort()\n",
    "\n",
    "for action_filename in list_dir:\n",
    "    print(format_action(action_filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pose sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../../outputs/fall/bbox_imposed/predictions/FallBackwardsS3.json'\n",
    "with open(file_path) as json_file:\n",
    "    pose_sequence = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pose_sequence))\n",
    "pose_sequence[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert into a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we convert the json in a dataframe. Each frame is a row in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_sequence[0]['instances'][0]['keypoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_dict = {}\n",
    "keypoints_list = pose_sequence[0]['instances'][0]['keypoints']\n",
    "\n",
    "for number, keypoint in enumerate(keypoints_list):\n",
    "    keypoints_dict[\"X_\" + id_joints_dict[number]] = keypoint[0]\n",
    "    keypoints_dict[\"Y_\" + id_joints_dict[number]] = keypoint[1]\n",
    "print(keypoints_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(data=keypoints_dict)\n",
    "ser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_list = []\n",
    "\n",
    "for i in range(2):\n",
    "    keypoints_dict = {}\n",
    "    keypoints_list = pose_sequence[i]['instances'][0]['keypoints']\n",
    "\n",
    "    for number, keypoint in enumerate(keypoints_list):\n",
    "        keypoints_dict[\"X_\" + id_joints_dict[number]] = keypoint[0]\n",
    "        keypoints_dict[\"Y_\" + id_joints_dict[number]] = keypoint[1]\n",
    "    \n",
    "    dic_list.append(keypoints_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(dic_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked, so now we gonna do it for the whole file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_sequence[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_list = []\n",
    "\n",
    "for i in range(len(pose_sequence)):\n",
    "    print(i)\n",
    "    keypoints_dict = {}\n",
    "    keypoints_list = pose_sequence[i]['instances'][0]['keypoints']\n",
    "\n",
    "    for number, keypoint in enumerate(keypoints_list):\n",
    "        keypoints_dict[\"X_\" + id_joints_dict[number]] = keypoint[0]\n",
    "        keypoints_dict[\"Y_\" + id_joints_dict[number]] = keypoint[1]\n",
    "    \n",
    "    dic_list.append(keypoints_dict)\n",
    "    \n",
    "df = pd.DataFrame.from_dict(dic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we add the labell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prodigy.components.db import connect\n",
    "\n",
    "db = connect()\n",
    "fall_data = db.get_dataset(\"fall_data\")\n",
    "\n",
    "def get_fall_label(file_path):\n",
    "    for i in range(len(fall_data)):\n",
    "        if fall_data[i]['text'] == file_path.split('/')[-1].split('.')[0]:\n",
    "            return fall_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../Data/Fall/Dataset_CAUCAFall/CAUCAFall/'\n",
    "images_path = format_action(file_path.split(\"/\")[-1])\n",
    "images_list = os.listdir(dataset_path + images_path)\n",
    "labels_file_list = [file for file in images_list if file.endswith(\".txt\")]\n",
    "labels_file_list.sort()\n",
    "labels_file_list.remove(\"classes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = []\n",
    "\n",
    "for file in labels_file_list:\n",
    "    with open(dataset_path + images_path + '/' + file) as f:\n",
    "        class_integer = int(f.readline()[0])\n",
    "        if class_integer == 0:\n",
    "            labels_list.append('Normal')\n",
    "        elif class_integer == 1:\n",
    "            labels_list.append('Lying down')\n",
    "\n",
    "labels_ser = pd.Series(labels_list, name='label')\n",
    "\n",
    "# On ajoute les chuttes s'il y en a\n",
    "label_dict = get_fall_label(file_path)\n",
    "\n",
    "if label_dict['audio_spans']:\n",
    "    # On récupère les informations de la vidéo, notamment le nombre d'images par seconde\n",
    "    video_folder_path = '../../Data/Fall/Dataset_CAUCAFall/video/'\n",
    "    info_dict = get_video_info_dict(video_folder_path + label_dict['text'] + '.mp4')\n",
    "    \n",
    "    start_frame = round(label_dict['audio_spans'][0]['start'] * info_dict['fps'])\n",
    "    end_frame = round(label_dict['audio_spans'][0]['end'] * info_dict['fps'])\n",
    "    labels_ser[start_frame:end_frame] = 'Fall'\n",
    "labels_ser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We add up everything in a final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_preds(file_path):\n",
    "\n",
    "    with open(file_path) as json_file:\n",
    "        pose_sequence = json.load(json_file)\n",
    "    \n",
    "    dic_list = []\n",
    "\n",
    "    for i in range(len(pose_sequence)):\n",
    "        keypoints_dict = {}\n",
    "\n",
    "        try:\n",
    "            keypoints_list = pose_sequence[i]['instances'][0]['keypoints']\n",
    "        except:\n",
    "            keypoints_list = [[np.nan, np.nan] for i in range(17)]\n",
    "\n",
    "        for number, keypoint in enumerate(keypoints_list):\n",
    "            keypoints_dict[\"X_\" + id_joints_dict[number]] = keypoint[0]\n",
    "            keypoints_dict[\"Y_\" + id_joints_dict[number]] = keypoint[1]\n",
    "        \n",
    "        dic_list.append(keypoints_dict)\n",
    "        \n",
    "    df = pd.DataFrame.from_dict(dic_list)\n",
    "\n",
    "    dataset_path = '../../Data/Fall/Dataset_CAUCAFall/CAUCAFall/'\n",
    "    images_path = format_action(file_path.split(\"/\")[-1])\n",
    "    images_list = os.listdir(dataset_path + images_path)\n",
    "    labels_file_list = [file for file in images_list if file.endswith(\".txt\")]\n",
    "    labels_file_list.sort()\n",
    "    labels_file_list.remove(\"classes.txt\")\n",
    "\n",
    "    labels_list = []\n",
    "\n",
    "    for file in labels_file_list:\n",
    "        with open(dataset_path + images_path + '/' + file) as f:\n",
    "            class_integer = int(f.readline()[0])\n",
    "            if class_integer == 0:\n",
    "                labels_list.append('Normal')\n",
    "            elif class_integer == 1:\n",
    "                labels_list.append('Lying down')\n",
    "\n",
    "    labels_ser = pd.Series(labels_list, name='label')\n",
    "    # On ajoute les chuttes s'il y en a\n",
    "    label_dict = get_fall_label(file_path)\n",
    "\n",
    "    if label_dict['audio_spans']:\n",
    "        # On récupère les informations de la vidéo, notamment le nombre d'images par seconde\n",
    "        video_folder_path = '../../Data/Fall/Dataset_CAUCAFall/video/'\n",
    "        info_dict = get_video_info_dict(video_folder_path + label_dict['text'] + '.mp4')\n",
    "        \n",
    "        start_frame = round(label_dict['audio_spans'][0]['start'] * info_dict['fps'])\n",
    "        end_frame = round(label_dict['audio_spans'][0]['end'] * info_dict['fps'])\n",
    "        labels_ser[start_frame:end_frame] = 'Fall'\n",
    "\n",
    "    df = pd.concat([df, labels_ser], axis=1)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df_from_preds(file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply this function to all predictions and save them as  csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_folder_path = '../../outputs/fall/bbox_imposed/predictions/'\n",
    "preds_files = os.listdir(preds_folder_path)\n",
    "preds_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_path = '../../Data/Fall/Dataset_CAUCAFall/Pose_sequences/'\n",
    "\n",
    "for preds_file in preds_files:\n",
    "    df = get_df_from_preds(preds_folder_path + preds_file)\n",
    "    df.to_csv(output_folder_path + preds_file.replace(\".json\", \".csv\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmposevenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
