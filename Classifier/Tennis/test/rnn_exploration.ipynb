{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_joints_dict = {0: 'nose',\n",
    "                1: 'left_eye',\n",
    "                2: 'right_eye',\n",
    "                3: 'left_ear',\n",
    "                4: 'right_ear',\n",
    "                5: 'left_shoulder',\n",
    "                6: 'right_shoulder',\n",
    "                7: 'left_elbow',\n",
    "                8: 'right_elbow',\n",
    "                9: 'left_wrist',\n",
    "                10: 'right_wrist',\n",
    "                11: 'left_hip',\n",
    "                12: 'right_hip',\n",
    "                13: 'left_knee',\n",
    "                14: 'right_knee',\n",
    "                15: 'left_ankle',\n",
    "                16: 'right_ankle'}\n",
    "joints_id_dict = {v: k for k, v in id_joints_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_preds(preds, instance_id=0):\n",
    "\n",
    "        \"\"\"\n",
    "        Get a dataframe from a json file containing the poses. The dataframe contains the coordinates of the joints of the instance_id-th.\n",
    "\n",
    "        Args:\n",
    "            preds (str or list): Path of the json file containing the poses or list of poses.\n",
    "\n",
    "        Returns:\n",
    "            df (pd.DataFrame): A dataframe of poses.\n",
    "        \"\"\"\n",
    "\n",
    "        if type(preds) == list:\n",
    "            pose_sequence = preds\n",
    "        else:\n",
    "            with open(preds) as json_file:\n",
    "                pose_sequence = json.load(json_file)\n",
    "        \n",
    "        dic_list = []\n",
    "\n",
    "        for i in range(len(pose_sequence)):\n",
    "            keypoints_dict = {}\n",
    "\n",
    "            try:\n",
    "                keypoints_list = pose_sequence[i]['instances'][instance_id]['keypoints']\n",
    "            except:\n",
    "                keypoints_list = [[np.nan, np.nan] for i in range(17)]\n",
    "\n",
    "            for number, keypoint in enumerate(keypoints_list):\n",
    "                keypoints_dict[\"X_\" + id_joints_dict[number]] = keypoint[0]\n",
    "                keypoints_dict[\"Y_\" + id_joints_dict[number]] = keypoint[1]\n",
    "            \n",
    "            dic_list.append(keypoints_dict)\n",
    "            \n",
    "        df = pd.DataFrame.from_dict(dic_list)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df_from_preds('../../../outputs/tennis/backhand/predictions/p1_backhand_s1.json')\n",
    "print('len(df):', len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward pass through RNN layer\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Select the last time step's output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Forward pass through fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward pass through LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Only take the output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test d'une pass-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres du modèle\n",
    "input_size =  df.shape[1]  # Nombre de features en entrée du RNN\n",
    "hidden_size = 128  # Taille de la couche cachée du RNN\n",
    "num_layers = 2  # Nombre de couches RNN empilées\n",
    "num_classes =  12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'instance du modèle\n",
    "model_test = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Vous pouvez également spécifier un dispositif (GPU ou CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.from_numpy(df.values).float().to(device)\n",
    "preds = model_test(tensor.unsqueeze(0))\n",
    "softmax = nn.Softmax(dim=1)\n",
    "softmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(preds, torch.tensor([5]).to(device))\n",
    "print('loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "input_size = df.shape[1]  # Dimension of input poses (X, Y for each joint)\n",
    "hidden_size = 64  # Number of LSTM units\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "num_classes = 6  # Number of output classes\n",
    "\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "model.to(device)\n",
    "# You can then define your loss function and optimizer, and train the model using your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(tensor.unsqueeze(0))\n",
    "softmax = nn.Softmax(dim=1)\n",
    "softmax(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TennisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, scaler=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.scaler = scaler\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.list_of_files = []\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit_transform(self.classes)\n",
    "\n",
    "        for classe in self.classes:\n",
    "            for file in os.listdir(os.path.join(root_dir, classe, 'predictions')):\n",
    "                self.list_of_files.append(os.path.join(root_dir, classe, 'predictions', file))\n",
    "        self.list_of_files.sort()\n",
    "    \n",
    "    def __len__(self):\n",
    "        number_of_files = 0\n",
    "        for classe in self.classes:\n",
    "            number_of_files += len(os.listdir(os.path.join(self.root_dir, classe, 'predictions')))\n",
    "        return number_of_files\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        file_name = self.list_of_files[idx]\n",
    "        df = get_df_from_preds(file_name)\n",
    "        label = file_name.split('/')[-3]\n",
    "        data = torch.from_numpy(df.values).float()\n",
    "\n",
    "        if self.scaler is not None:\n",
    "            data = self.scaler.transform(data)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return data, self.label_encoder.transform([label])[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TennisDataset('../../../outputs/tennis_no_vis_cluster')\n",
    "print('Number of files in the dataset:', len(dataset))\n",
    "print('Classes:', dataset.classes)\n",
    "print('List of files:', dataset.list_of_files[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.label_encoder.inverse_transform([dataset[1900][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split en test set et train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indexes = list(range(len(dataset)))\n",
    "\n",
    "num_test = int(len(dataset) * 0.2)\n",
    "num_train = len(dataset) - num_test\n",
    "\n",
    "train_indexes = list(np.random.choice(all_indexes, num_train, replace=False))\n",
    "test_indexes = list(set(all_indexes) - set(train_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(dataset, train_indexes)\n",
    "test_dataset = Subset(dataset, test_indexes)\n",
    "print('Number of files in the train dataset:', len(train_dataset))\n",
    "print('Number of files in the test dataset:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the scaler on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    train_data.append(train_dataset[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(torch.cat(train_data).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before scaling:', train_dataset[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TennisDataset('../../../outputs/tennis_no_vis_cluster', scaler=scaler, transform=lambda x: torch.from_numpy(x).float())\n",
    "train_dataset = Subset(dataset, train_indexes)\n",
    "test_dataset = Subset(dataset, test_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After scaling:', train_dataset[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size =  34\n",
    "hidden_size = 64  # Taille de la couche cachée du RNN\n",
    "num_layers = 2  # Nombre de couches RNN empilées\n",
    "num_classes =  6\n",
    "\n",
    "model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "model.to(device)\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size =  34\n",
    "hidden_size = 124 \n",
    "num_layers = 1\n",
    "num_classes =  6\n",
    "\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "model.to(device)\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création des dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # batch est une liste de tuples (data, label)\n",
    "    data, labels = zip(*batch)\n",
    "    \n",
    "    # Inverser chaque séquence\n",
    "    reversed_data = [torch.flip(seq, [0]) for seq in data]\n",
    "    \n",
    "    # Remplir les séquences inversées pour qu'elles aient la même longueur\n",
    "    padded_data = pad_sequence(reversed_data, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Inverser à nouveau chaque séquence du résultat pour avoir le padding au début\n",
    "    data = [torch.flip(seq, [0]) for seq in padded_data]\n",
    "    data = torch.stack(data)\n",
    "    \n",
    "    labels = torch.tensor(labels)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch, (sequences, targets) in enumerate(train_dataloader):\n",
    "        sequences = sequences.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # scheduler.step(loss)\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                .format(epoch, epochs, batch, len(train_dataloader), loss.item()))\n",
    "        \n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, target in test_dataloader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "        print('Test Accuracy of the model on the {} test videos: {} %'.format(len(test_dataset), 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in test_dataloader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the {} test videos: {} %'.format(len(test_dataset), 100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmposevenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
